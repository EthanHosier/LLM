{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3e9f46-781a-430e-b550-b7140375c09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "batch_size = 64 #32\n",
    "block_size = 128 #how many blocks in parallel\n",
    "learning_rate = 3e-4\n",
    "max_iters = 3000\n",
    "eval_iters = 100\n",
    "dropout = 0.2 #dropout random neurons of network = don't overfit\n",
    "n_embd = 384\n",
    "n_layer = 8 #1\n",
    "n_head = 8 #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d05757f-322d-49a9-b111-0c9c74c3480b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x00',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '\\x7f',\n",
       " '\\x80',\n",
       " '\\x81',\n",
       " '\\x82',\n",
       " '\\x83',\n",
       " '\\x84',\n",
       " '\\x86',\n",
       " '\\x87',\n",
       " '\\x88',\n",
       " '\\x89',\n",
       " '\\x8a',\n",
       " '\\x8b',\n",
       " '\\x8c',\n",
       " '\\x8d',\n",
       " '\\x8e',\n",
       " '\\x8f',\n",
       " '\\x90',\n",
       " '\\x91',\n",
       " '\\x92',\n",
       " '\\x93',\n",
       " '\\x94',\n",
       " '\\x95',\n",
       " '\\x96',\n",
       " '\\x97',\n",
       " '\\x98',\n",
       " '\\x99',\n",
       " '\\x9a',\n",
       " '\\x9b',\n",
       " '\\x9c',\n",
       " '\\x9d',\n",
       " '\\x9e',\n",
       " '\\x9f',\n",
       " '¡',\n",
       " '¢',\n",
       " '£',\n",
       " '¤',\n",
       " '¥',\n",
       " '¦',\n",
       " '§',\n",
       " '¨',\n",
       " '©',\n",
       " 'ª',\n",
       " '«',\n",
       " '¬',\n",
       " '\\xad',\n",
       " '®',\n",
       " '¯',\n",
       " '°',\n",
       " '±',\n",
       " '²',\n",
       " '³',\n",
       " '´',\n",
       " 'µ',\n",
       " '¶',\n",
       " '·',\n",
       " '¸',\n",
       " '¹',\n",
       " 'º',\n",
       " '»',\n",
       " '¼',\n",
       " '½',\n",
       " '¾',\n",
       " '¿',\n",
       " 'À',\n",
       " 'Á',\n",
       " 'Â',\n",
       " 'Ã',\n",
       " 'Ä',\n",
       " 'Å',\n",
       " 'Æ',\n",
       " 'Ç',\n",
       " 'È',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'Ë',\n",
       " 'Ì',\n",
       " 'Í',\n",
       " 'Î',\n",
       " 'Ï',\n",
       " 'Ð',\n",
       " 'Ñ',\n",
       " 'Ò',\n",
       " 'Ó',\n",
       " 'Ô',\n",
       " 'Õ',\n",
       " 'Ö',\n",
       " '×',\n",
       " 'Ø',\n",
       " 'Ù',\n",
       " 'Ú',\n",
       " 'Û',\n",
       " 'Ü',\n",
       " 'Ý',\n",
       " 'Þ',\n",
       " 'ß',\n",
       " 'à',\n",
       " 'á',\n",
       " 'â',\n",
       " 'ã',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'æ',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'ë',\n",
       " 'ì',\n",
       " 'í',\n",
       " 'î',\n",
       " 'ï',\n",
       " 'ð',\n",
       " 'ñ',\n",
       " 'ò',\n",
       " 'ó',\n",
       " 'ô',\n",
       " 'õ',\n",
       " 'ö',\n",
       " '÷',\n",
       " 'ø',\n",
       " 'ù',\n",
       " 'ú',\n",
       " 'û',\n",
       " 'ü',\n",
       " 'ý',\n",
       " 'þ',\n",
       " 'ÿ',\n",
       " 'Ā',\n",
       " 'ā',\n",
       " 'Ă',\n",
       " 'ă',\n",
       " 'Ą',\n",
       " 'ą',\n",
       " 'Ć',\n",
       " 'ć',\n",
       " 'Ĉ',\n",
       " 'ĉ',\n",
       " 'Ċ',\n",
       " 'ċ',\n",
       " 'Č',\n",
       " 'č',\n",
       " 'Ď',\n",
       " 'ď',\n",
       " 'Đ',\n",
       " 'đ',\n",
       " 'Ē',\n",
       " 'ē',\n",
       " 'Ĕ',\n",
       " 'ĕ',\n",
       " 'Ė',\n",
       " 'ė',\n",
       " 'Ę',\n",
       " 'ę',\n",
       " 'Ě',\n",
       " 'ě',\n",
       " 'Ĝ',\n",
       " 'ĝ',\n",
       " 'Ğ',\n",
       " 'ğ',\n",
       " 'Ġ',\n",
       " 'ġ',\n",
       " 'Ģ',\n",
       " 'ģ',\n",
       " 'Ĥ',\n",
       " 'ĥ',\n",
       " 'Ħ',\n",
       " 'ħ',\n",
       " 'Ĩ',\n",
       " 'ĩ',\n",
       " 'Ī',\n",
       " 'ī',\n",
       " 'Ĭ',\n",
       " 'ĭ',\n",
       " 'Į',\n",
       " 'į',\n",
       " 'İ',\n",
       " 'ı',\n",
       " 'Ĳ',\n",
       " 'ĳ',\n",
       " 'Ĵ',\n",
       " 'ĵ',\n",
       " 'Ķ',\n",
       " 'ķ',\n",
       " 'ĸ',\n",
       " 'Ĺ',\n",
       " 'ĺ',\n",
       " 'Ļ',\n",
       " 'ļ',\n",
       " 'Ľ',\n",
       " 'ľ',\n",
       " 'Ŀ',\n",
       " 'ŀ',\n",
       " 'Ł',\n",
       " 'ł',\n",
       " 'Ń',\n",
       " 'ń',\n",
       " 'Ņ',\n",
       " 'ņ',\n",
       " 'Ň',\n",
       " 'ň',\n",
       " 'ŉ',\n",
       " 'Ŋ',\n",
       " 'ŋ',\n",
       " 'Ō',\n",
       " 'ō',\n",
       " 'Ŏ',\n",
       " 'ŏ',\n",
       " 'Ő',\n",
       " 'ő',\n",
       " 'Œ',\n",
       " 'œ',\n",
       " 'Ŕ',\n",
       " 'ŕ',\n",
       " 'Ŗ',\n",
       " 'ŗ',\n",
       " 'Ř',\n",
       " 'ř',\n",
       " 'Ś',\n",
       " 'ś',\n",
       " 'Ŝ',\n",
       " 'ŝ',\n",
       " 'Ş',\n",
       " 'ş',\n",
       " 'Š',\n",
       " 'š',\n",
       " 'Ţ',\n",
       " 'ţ',\n",
       " 'Ť',\n",
       " 'ť',\n",
       " 'Ŧ',\n",
       " 'ŧ',\n",
       " 'Ũ',\n",
       " 'ũ',\n",
       " 'Ū',\n",
       " 'ū',\n",
       " 'Ŭ',\n",
       " 'ŭ',\n",
       " 'Ů',\n",
       " 'ů',\n",
       " 'Ű',\n",
       " 'ű',\n",
       " 'Ų',\n",
       " 'ų',\n",
       " 'Ŵ',\n",
       " 'ŵ',\n",
       " 'Ŷ',\n",
       " 'ŷ',\n",
       " 'Ÿ',\n",
       " 'Ź',\n",
       " 'ź',\n",
       " 'Ż',\n",
       " 'ż',\n",
       " 'Ž',\n",
       " 'ž',\n",
       " 'ſ',\n",
       " 'ƀ',\n",
       " 'Ɓ',\n",
       " 'Ƃ',\n",
       " 'ƃ',\n",
       " 'Ƅ',\n",
       " 'ƅ',\n",
       " 'Ɔ',\n",
       " 'Ƈ',\n",
       " 'ƈ',\n",
       " 'Ɖ',\n",
       " 'Ɗ',\n",
       " 'Ƌ',\n",
       " 'ƌ',\n",
       " 'ƍ',\n",
       " 'Ǝ',\n",
       " 'Ə',\n",
       " 'Ɛ',\n",
       " 'Ƒ',\n",
       " 'ƒ',\n",
       " 'Ɠ',\n",
       " 'Ɣ',\n",
       " 'ƕ',\n",
       " 'Ɩ',\n",
       " 'Ɨ',\n",
       " 'Ƙ',\n",
       " 'ƙ',\n",
       " 'ƚ',\n",
       " 'ƛ',\n",
       " 'Ɯ',\n",
       " 'Ɲ',\n",
       " 'ƞ',\n",
       " 'Ɵ',\n",
       " 'Ơ',\n",
       " 'ơ',\n",
       " 'Ƣ',\n",
       " 'ƣ',\n",
       " 'Ƥ',\n",
       " 'ƥ',\n",
       " 'Ʀ',\n",
       " 'Ƨ',\n",
       " 'ƨ',\n",
       " 'Ʃ',\n",
       " 'ƪ',\n",
       " 'ƫ',\n",
       " 'Ƭ',\n",
       " 'ƭ',\n",
       " 'Ʈ',\n",
       " 'Ư',\n",
       " 'ư',\n",
       " 'Ʊ',\n",
       " 'Ʋ',\n",
       " 'Ƴ',\n",
       " 'ƴ',\n",
       " 'Ƶ',\n",
       " 'ƶ',\n",
       " 'Ʒ',\n",
       " 'Ƹ',\n",
       " 'ƹ',\n",
       " 'ƺ',\n",
       " 'ƻ',\n",
       " 'Ƽ',\n",
       " 'ƽ',\n",
       " 'ƾ',\n",
       " 'ƿ',\n",
       " 'ǀ',\n",
       " 'ǁ',\n",
       " 'ǂ',\n",
       " 'ǃ',\n",
       " 'Ǆ',\n",
       " 'ǅ',\n",
       " 'ǆ',\n",
       " 'Ǉ',\n",
       " 'ǈ',\n",
       " 'ǉ',\n",
       " 'Ǌ',\n",
       " 'ǋ',\n",
       " 'ǌ',\n",
       " 'Ǎ',\n",
       " 'ǎ',\n",
       " 'Ǐ',\n",
       " 'ǐ',\n",
       " 'Ǒ',\n",
       " 'ǒ',\n",
       " 'Ǔ',\n",
       " 'ǔ',\n",
       " 'Ǖ',\n",
       " 'ǖ',\n",
       " 'Ǘ',\n",
       " 'ǘ',\n",
       " 'Ǚ',\n",
       " 'ǚ',\n",
       " 'Ǜ',\n",
       " 'ǜ',\n",
       " 'ǝ',\n",
       " 'Ǟ',\n",
       " 'ǟ',\n",
       " 'Ǡ',\n",
       " 'ǡ',\n",
       " 'Ǣ',\n",
       " 'ǣ',\n",
       " 'Ǥ',\n",
       " 'ǥ',\n",
       " 'Ǧ',\n",
       " 'ǧ',\n",
       " 'Ǩ',\n",
       " 'ǩ',\n",
       " 'Ǫ',\n",
       " 'ǫ',\n",
       " 'Ǭ',\n",
       " 'ǭ',\n",
       " 'Ǯ',\n",
       " 'ǯ',\n",
       " 'ǰ',\n",
       " 'Ǳ',\n",
       " 'ǲ',\n",
       " 'ǳ',\n",
       " 'Ǵ',\n",
       " 'ǵ',\n",
       " 'Ƕ',\n",
       " 'Ƿ',\n",
       " 'Ǹ',\n",
       " 'ǹ',\n",
       " 'Ǻ',\n",
       " 'ǻ',\n",
       " 'Ǽ',\n",
       " 'ǽ',\n",
       " 'Ǿ',\n",
       " 'ǿ',\n",
       " 'Ȁ',\n",
       " 'ȁ',\n",
       " 'Ȃ',\n",
       " 'ȃ',\n",
       " 'Ȅ',\n",
       " 'ȅ',\n",
       " 'Ȇ',\n",
       " 'ȇ',\n",
       " 'Ȉ',\n",
       " 'ȉ',\n",
       " 'Ȋ',\n",
       " 'ȋ',\n",
       " 'Ȍ',\n",
       " 'ȍ',\n",
       " 'Ȏ',\n",
       " 'ȏ',\n",
       " 'Ȑ',\n",
       " 'ȑ',\n",
       " 'Ȓ',\n",
       " 'ȓ',\n",
       " 'Ȕ',\n",
       " 'ȕ',\n",
       " 'Ȗ',\n",
       " 'ȗ',\n",
       " 'Ș',\n",
       " 'ș',\n",
       " 'Ț',\n",
       " 'ț',\n",
       " 'Ȝ',\n",
       " 'ȝ',\n",
       " 'Ȟ',\n",
       " 'ȟ',\n",
       " 'Ƞ',\n",
       " 'ȡ',\n",
       " 'Ȣ',\n",
       " 'ȣ',\n",
       " 'Ȥ',\n",
       " 'ȥ',\n",
       " 'Ȧ',\n",
       " 'ȧ',\n",
       " 'Ȩ',\n",
       " 'ȩ',\n",
       " 'Ȫ',\n",
       " 'ȫ',\n",
       " 'Ȭ',\n",
       " 'ȭ',\n",
       " 'Ȯ',\n",
       " 'ȯ',\n",
       " 'Ȱ',\n",
       " 'ȱ',\n",
       " 'Ȳ',\n",
       " 'ȳ',\n",
       " 'ȴ',\n",
       " 'ȵ',\n",
       " 'ȶ',\n",
       " 'ȷ',\n",
       " 'ȸ',\n",
       " 'ȹ',\n",
       " 'Ⱥ',\n",
       " 'Ȼ',\n",
       " 'ȼ',\n",
       " 'Ƚ',\n",
       " 'Ⱦ',\n",
       " 'ȿ',\n",
       " 'ɀ',\n",
       " 'Ɂ',\n",
       " 'ɂ',\n",
       " 'Ƀ',\n",
       " 'Ʉ',\n",
       " 'Ʌ',\n",
       " 'Ɇ',\n",
       " 'ɇ',\n",
       " 'Ɉ',\n",
       " 'ɉ',\n",
       " 'Ɋ',\n",
       " 'ɋ',\n",
       " 'Ɍ',\n",
       " 'ɍ',\n",
       " 'Ɏ',\n",
       " 'ɏ',\n",
       " 'ɐ',\n",
       " 'ɑ',\n",
       " 'ɒ',\n",
       " 'ɓ',\n",
       " 'ɔ',\n",
       " 'ɕ',\n",
       " 'ɖ',\n",
       " 'ɗ',\n",
       " 'ɘ',\n",
       " 'ə',\n",
       " 'ɚ',\n",
       " 'ɛ',\n",
       " 'ɜ',\n",
       " 'ɝ',\n",
       " 'ɞ',\n",
       " 'ɟ',\n",
       " 'ɠ',\n",
       " 'ɡ',\n",
       " 'ɢ',\n",
       " 'ɣ',\n",
       " 'ɤ',\n",
       " 'ɥ',\n",
       " 'ɦ',\n",
       " 'ɧ',\n",
       " 'ɨ',\n",
       " 'ɩ',\n",
       " 'ɪ',\n",
       " 'ɫ',\n",
       " 'ɬ',\n",
       " 'ɭ',\n",
       " 'ɮ',\n",
       " 'ɯ',\n",
       " 'ɰ',\n",
       " 'ɱ',\n",
       " 'ɲ',\n",
       " 'ɳ',\n",
       " 'ɴ',\n",
       " 'ɵ',\n",
       " 'ɶ',\n",
       " 'ɷ',\n",
       " 'ɸ',\n",
       " 'ɹ',\n",
       " 'ɺ',\n",
       " 'ɻ',\n",
       " 'ɼ',\n",
       " 'ɽ',\n",
       " 'ɾ',\n",
       " 'ɿ',\n",
       " 'ʀ',\n",
       " 'ʁ',\n",
       " 'ʂ',\n",
       " 'ʃ',\n",
       " 'ʄ',\n",
       " 'ʅ',\n",
       " 'ʆ',\n",
       " 'ʇ',\n",
       " 'ʈ',\n",
       " 'ʉ',\n",
       " 'ʊ',\n",
       " 'ʋ',\n",
       " 'ʌ',\n",
       " 'ʍ',\n",
       " 'ʎ',\n",
       " 'ʏ',\n",
       " 'ʐ',\n",
       " 'ʑ',\n",
       " 'ʒ',\n",
       " 'ʓ',\n",
       " 'ʔ',\n",
       " 'ʕ',\n",
       " 'ʖ',\n",
       " 'ʗ',\n",
       " 'ʘ',\n",
       " 'ʙ',\n",
       " 'ʚ',\n",
       " 'ʛ',\n",
       " 'ʜ',\n",
       " 'ʝ',\n",
       " 'ʞ',\n",
       " 'ʟ',\n",
       " 'ʠ',\n",
       " 'ʡ',\n",
       " 'ʢ',\n",
       " 'ʣ',\n",
       " 'ʤ',\n",
       " 'ʥ',\n",
       " 'ʦ',\n",
       " 'ʧ',\n",
       " 'ʨ',\n",
       " 'ʩ',\n",
       " 'ʪ',\n",
       " 'ʫ',\n",
       " 'ʬ',\n",
       " 'ʭ',\n",
       " 'ʯ',\n",
       " 'ʰ',\n",
       " 'ʱ',\n",
       " 'ʲ',\n",
       " 'ʳ',\n",
       " 'ʴ',\n",
       " 'ʵ',\n",
       " 'ʶ',\n",
       " 'ʷ',\n",
       " 'ʸ',\n",
       " 'ʹ',\n",
       " 'ʺ',\n",
       " 'ʻ',\n",
       " 'ʼ',\n",
       " 'ʽ',\n",
       " 'ʾ',\n",
       " 'ʿ',\n",
       " 'ˀ',\n",
       " 'ˁ',\n",
       " '˂',\n",
       " '˃',\n",
       " '˄',\n",
       " '˅',\n",
       " 'ˆ',\n",
       " 'ˇ',\n",
       " 'ˈ',\n",
       " 'ˉ',\n",
       " 'ˊ',\n",
       " 'ˋ',\n",
       " 'ˌ',\n",
       " 'ˍ',\n",
       " 'ˎ',\n",
       " 'ˏ',\n",
       " 'ː',\n",
       " 'ˑ',\n",
       " '˒',\n",
       " '˓',\n",
       " '˔',\n",
       " '˕',\n",
       " '˖',\n",
       " '˗',\n",
       " '˘',\n",
       " '˙',\n",
       " '˚',\n",
       " '˛',\n",
       " '˜',\n",
       " '˝',\n",
       " '˞',\n",
       " '˟',\n",
       " 'ˠ',\n",
       " 'ˡ',\n",
       " 'ˢ',\n",
       " 'ˣ',\n",
       " 'ˤ',\n",
       " '˥',\n",
       " '˦',\n",
       " '˧',\n",
       " '˨',\n",
       " '˩',\n",
       " '˪',\n",
       " '˫',\n",
       " 'ˬ',\n",
       " '˭',\n",
       " 'ˮ',\n",
       " '˰',\n",
       " '˱',\n",
       " '˲',\n",
       " '˳',\n",
       " '˴',\n",
       " '˵',\n",
       " '˶',\n",
       " '˸',\n",
       " '˹',\n",
       " '˺',\n",
       " '˻',\n",
       " '˾',\n",
       " '̀',\n",
       " '́',\n",
       " '̂',\n",
       " '̃',\n",
       " '̄',\n",
       " '̅',\n",
       " '̆',\n",
       " '̇',\n",
       " '̈',\n",
       " '̉',\n",
       " '̊',\n",
       " '̋',\n",
       " '̌',\n",
       " '̍',\n",
       " '̎',\n",
       " '̏',\n",
       " '̐',\n",
       " '̑',\n",
       " '̒',\n",
       " '̓',\n",
       " '̔',\n",
       " '̕',\n",
       " '̖',\n",
       " '̗',\n",
       " '̘',\n",
       " '̙',\n",
       " '̚',\n",
       " '̛',\n",
       " '̜',\n",
       " '̝',\n",
       " '̞',\n",
       " '̟',\n",
       " '̠',\n",
       " '̡',\n",
       " '̢',\n",
       " '̣',\n",
       " '̤',\n",
       " '̥',\n",
       " '̦',\n",
       " '̧',\n",
       " '̨',\n",
       " '̩',\n",
       " '̪',\n",
       " '̫',\n",
       " '̬',\n",
       " '̭',\n",
       " '̮',\n",
       " '̯',\n",
       " '̰',\n",
       " '̱',\n",
       " '̲',\n",
       " '̳',\n",
       " '̴',\n",
       " '̵',\n",
       " '̶',\n",
       " '̷',\n",
       " '̸',\n",
       " '̹',\n",
       " '̺',\n",
       " '̻',\n",
       " '̼',\n",
       " '̽',\n",
       " '̾',\n",
       " '̿',\n",
       " '̀',\n",
       " '́',\n",
       " '͂',\n",
       " '̓',\n",
       " '̈́',\n",
       " 'ͅ',\n",
       " '͆',\n",
       " '͇',\n",
       " '͈',\n",
       " '͉',\n",
       " '͊',\n",
       " '͋',\n",
       " '͌',\n",
       " '͍',\n",
       " '͎',\n",
       " '͏',\n",
       " '͐',\n",
       " '͑',\n",
       " '͒',\n",
       " '͓',\n",
       " '͔',\n",
       " '͕',\n",
       " '͖',\n",
       " '͗',\n",
       " '͘',\n",
       " '͙',\n",
       " '͚',\n",
       " '͛',\n",
       " '͜',\n",
       " '͝',\n",
       " '͞',\n",
       " '͟',\n",
       " '͠',\n",
       " '͡',\n",
       " '͢',\n",
       " 'ͣ',\n",
       " 'ͤ',\n",
       " 'ͥ',\n",
       " 'ͦ',\n",
       " 'ͧ',\n",
       " 'ͨ',\n",
       " 'ͩ',\n",
       " 'ͪ',\n",
       " 'ͫ',\n",
       " 'ͬ',\n",
       " 'ͭ',\n",
       " 'ͮ',\n",
       " 'ͯ',\n",
       " 'Ͱ',\n",
       " 'ͱ',\n",
       " 'Ͳ',\n",
       " 'ͳ',\n",
       " 'ʹ',\n",
       " '͵',\n",
       " 'Ͷ',\n",
       " 'ͷ',\n",
       " '\\u0378',\n",
       " '\\u0379',\n",
       " 'ͺ',\n",
       " 'ͻ',\n",
       " 'ͼ',\n",
       " 'ͽ',\n",
       " ';',\n",
       " 'Ϳ',\n",
       " '\\u0380',\n",
       " '\\u0381',\n",
       " '\\u0382',\n",
       " '\\u0383',\n",
       " '΄',\n",
       " '΅',\n",
       " 'Ά',\n",
       " '·',\n",
       " 'Έ',\n",
       " 'Ή',\n",
       " 'Ί',\n",
       " '\\u038b',\n",
       " 'Ό',\n",
       " '\\u038d',\n",
       " 'Ύ',\n",
       " 'Ώ',\n",
       " 'ΐ',\n",
       " 'Α',\n",
       " 'Β',\n",
       " 'Γ',\n",
       " 'Δ',\n",
       " 'Ε',\n",
       " 'Ζ',\n",
       " 'Η',\n",
       " 'Θ',\n",
       " 'Ι',\n",
       " 'Κ',\n",
       " 'Λ',\n",
       " 'Μ',\n",
       " 'Ν',\n",
       " 'Ξ',\n",
       " 'Ο',\n",
       " 'Π',\n",
       " 'Ρ',\n",
       " '\\u03a2',\n",
       " 'Σ',\n",
       " 'Τ',\n",
       " 'Υ',\n",
       " 'Φ',\n",
       " 'Χ',\n",
       " 'Ψ',\n",
       " 'Ω',\n",
       " 'Ϊ',\n",
       " 'Ϋ',\n",
       " 'ά',\n",
       " 'έ',\n",
       " 'ή',\n",
       " 'ί',\n",
       " 'ΰ',\n",
       " 'α',\n",
       " 'β',\n",
       " 'γ',\n",
       " 'δ',\n",
       " 'ε',\n",
       " 'ζ',\n",
       " 'η',\n",
       " 'θ',\n",
       " 'ι',\n",
       " 'κ',\n",
       " 'λ',\n",
       " 'μ',\n",
       " 'ν',\n",
       " 'ξ',\n",
       " 'ο',\n",
       " 'π',\n",
       " 'ρ',\n",
       " 'ς',\n",
       " 'σ',\n",
       " 'τ',\n",
       " 'υ',\n",
       " 'φ',\n",
       " 'χ',\n",
       " 'ψ',\n",
       " 'ω',\n",
       " 'ϊ',\n",
       " 'ϋ',\n",
       " 'ό',\n",
       " 'ύ',\n",
       " 'ώ',\n",
       " 'Ϗ',\n",
       " 'ϐ',\n",
       " 'ϑ',\n",
       " 'ϒ',\n",
       " 'ϓ',\n",
       " 'ϔ',\n",
       " 'ϕ',\n",
       " 'ϖ',\n",
       " 'ϗ',\n",
       " 'Ϙ',\n",
       " 'ϙ',\n",
       " 'Ϛ',\n",
       " 'ϛ',\n",
       " 'Ϝ',\n",
       " 'ϝ',\n",
       " 'Ϟ',\n",
       " 'ϟ',\n",
       " 'Ϡ',\n",
       " 'ϡ',\n",
       " 'Ϣ',\n",
       " 'ϣ',\n",
       " 'Ϥ',\n",
       " 'ϥ',\n",
       " 'Ϧ',\n",
       " 'ϧ',\n",
       " 'Ϩ',\n",
       " 'ϩ',\n",
       " 'Ϫ',\n",
       " 'ϫ',\n",
       " 'Ϭ',\n",
       " 'ϭ',\n",
       " 'Ϯ',\n",
       " 'ϯ',\n",
       " 'ϰ',\n",
       " 'ϱ',\n",
       " 'ϲ',\n",
       " 'ϳ',\n",
       " 'ϴ',\n",
       " 'ϵ',\n",
       " '϶',\n",
       " 'Ϸ',\n",
       " 'ϸ',\n",
       " 'Ϲ',\n",
       " 'Ϻ',\n",
       " 'ϻ',\n",
       " 'ϼ',\n",
       " 'Ͻ',\n",
       " 'Ͼ',\n",
       " 'Ͽ',\n",
       " 'Ѐ',\n",
       " 'Ё',\n",
       " 'Ђ',\n",
       " 'Ѓ',\n",
       " 'Є',\n",
       " 'Ѕ',\n",
       " 'І',\n",
       " 'Ї',\n",
       " 'Ј',\n",
       " 'Љ',\n",
       " 'Њ',\n",
       " 'Ћ',\n",
       " 'Ќ',\n",
       " 'Ѝ',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = \"\"\n",
    "with open('vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "    \n",
    "vocab_size = len(chars)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd3b377-790b-4dcc-b886-4ce47a25f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11599,     1, 18365,     1,  9086,     1, 16102,     1, 22428,     1,\n",
      "        20766,     1,  5317,     1, 14587,     1, 18693,     1, 20938,     1,\n",
      "         4646,     1, 11798,     1,  6731,     1, 31293,     1, 26809,     1,\n",
      "        12670,     1, 23609,     1, 16201,     1,   140,     1, 30670,     1,\n",
      "         8700,     1, 11806,     1, 11705,     1, 17074,     1, 13754,     1,\n",
      "        31140,     1, 17791,     1, 12024,     1, 28725,     1, 25863,     1,\n",
      "        25933,     1,  1000,     1, 13916,     1,  9396,     1, 25763,     1,\n",
      "        16526,     1, 24074,     1,  4606,     1,  4618,     1, 12496,     1,\n",
      "        25598,     1, 15397,     1, 28768,     1, 11272,     1,    70,     1,\n",
      "        14181,     1, 15754,     1,    29,     1, 18432,     1,  8707,     1])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i, ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s];\n",
    "decode = lambda l: \"\".join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1c4eb3-680a-49ae-961b-8d97ffddb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[  71,   70,   75,  ...,   81,   78,   75],\n",
      "        [  16,    1,    1,  ...,    2,   67,   80],\n",
      "        [  69,   71,    2,  ...,   86,   71,   78],\n",
      "        ...,\n",
      "        [  69,   74,   75,  ...,   74,   14, 6187],\n",
      "        [  78,   67,   80,  ...,   72,    2,   82],\n",
      "        [  21,   67,   22,  ...,   18,   18,   18]], device='cuda:0')\n",
      "targets: \n",
      "tensor([[  70,   75,   85,  ...,   78,   75,   86],\n",
      "        [   1,    1,   57,  ...,   67,   80,    2],\n",
      "        [  71,    2,   75,  ...,   71,   78,   78],\n",
      "        ...,\n",
      "        [  74,   75,   80,  ...,   14, 6187,    2],\n",
      "        [  67,   80,    2,  ...,    2,   82,   67],\n",
      "        [  67,   22,   20,  ...,   18,   18,   18]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# memory map for using small snippets of text from a single file of any size\n",
    "def get_random_chunk(split):\n",
    "    filename = \"output_train.txt\" if split == 'train' else \"output_val.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) #push to gpu\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(\"inputs: \")\n",
    "print(x)\n",
    "\n",
    "print (\"targets: \")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e5c6958-49f6-4751-8dac-ea6ee8a72331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decorator makes pytorch not use gradients at all in it\n",
    "# = reduces computation / memory usage\n",
    "#NO TRAINING IS HAPPENING HERE\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "\n",
    "    # puts model into eval mode - for evaluating / testing\n",
    "    # ** stuff disabled\n",
    "    # want to see how well model actually performs\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "        \n",
    "        # puts model into training mode\n",
    "        # ** weights + biases update during this phase\n",
    "        # ** dropout + batch normalisation active\n",
    "        model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1434c3f5-d788-4aa8-a5ba-d1dc5f2bf232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #save 'no look ahead masking' to actual model state = reduce overhead\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, hs)\n",
    "        q = self.query(x) # (B, T, hs)\n",
    "\n",
    "        #compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 #(B, T, hs) @ (B, hs, T) -> (B. T. T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        #perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # heads in parallel\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h2, h2, h2 ... ])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer fllowed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.next = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout), #makes certain percentage of neurons 0 = prevents overfitting\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.next(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head #how many features captured by each head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd) #linear, relu, linear\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y) #add and norm (residual connection)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y) #add and norm\n",
    "        return x\n",
    "\n",
    "class GPTLanuageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) #so don't lose info about positioning of tokens\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) #how many sequential decoder blocks / layers\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # Final layer norm, helps to converge better\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # Projecting from n_embd to vocab_size (for use with softmax)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # initialise weights around standard deviation\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "\n",
    "        B, T = index.shape\n",
    "        \n",
    "        #idx and targets are both (B, T) tensors of integers\n",
    "        tok_emb = self.token_embedding_table(index) #(B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #(T, C), torch.arange is indices from 0 to length(T) - 1\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) #(B, T, vocab_size), to put in softmax\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # batch size, ('time steps' or 'sequence length'), 'channels' = dimensionality of vectors =  vocab_size  \n",
    "            B, T, C = logits.shape\n",
    "            \n",
    "            #flatten, now where the characters are in their individual sequences doesn't matter\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            #compares the vectors from embedding table to the truth value (truth vectors automatically generated, pytorch expects only the actual truth val here)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "        \n",
    "    #we're not training here, just using existing model\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            \n",
    "            # focus only on the last time step AS ONLY WANT NEXT CHARACTER\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = GPTLanuageModel(vocab_size)\n",
    "print(\"loading model parameters...\")\n",
    "with open('model-01.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "print(\"loaded successfully\")\n",
    "m = model.to(device) #push to GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67bbf26-ce25-400d-852d-829a86456719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 10.440, val loss: 10.437, time elapsed: 69.97 seconds\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1006.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 354.69 MiB is free. Including non-PyTorch memory, this process has 3.46 GiB memory in use. Of the allocated memory 2.92 GiB is allocated by PyTorch, and 487.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#xb = index / inputs, yb = target\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#evaluate the loss\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#so previous gradients don't affect current gradient\u001b[39;00m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 124\u001b[0m, in \u001b[0;36mGPTLanuageModel.forward\u001b[0;34m(self, index, targets)\u001b[0m\n\u001b[1;32m    121\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m#compares the vectors from embedding table to the truth value (truth vectors automatically generated, pytorch expects only the actual truth val here)\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[0;32m~/Other/envs/cuda_env/lib/python3.8/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1006.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 354.69 MiB is free. Including non-PyTorch memory, this process has 3.46 GiB memory in use. Of the allocated memory 2.92 GiB is allocated by PyTorch, and 487.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#pytorch optimizer:\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "start_time = time.time()  # Start time measurement before the loop\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        current_time = time.time()  # Current time at this iteration\n",
    "        duration = current_time - start_time  # Duration since the start of the loop\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}, time elapsed: {duration:.2f} seconds\")\n",
    "\n",
    "    #sample a batch of data\n",
    "    xb, yb = get_batch('train') #xb = index / inputs, yb = target\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "\n",
    "    #so previous gradients don't affect current gradient\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    #computes gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #updates model's weights based on gradients\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
